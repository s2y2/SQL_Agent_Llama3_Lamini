{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up usage of .env file (storage of Lamini API Key)\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up of imports, logging, database, a class for arguments\n",
    "\n",
    "import logging\n",
    "import random\n",
    "from typing import AsyncIterator, Iterator, Union\n",
    "import sqlite3\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import lamini \n",
    "from lamini.generation.base_prompt_object import PromptObject\n",
    "from lamini.generation.generation_node import GenerationNode\n",
    "from lamini.generation.generation_pipeline import GenerationPipeline\n",
    "from util.get_schema import get_schema, get_schema_altered\n",
    "from util.make_llama_3_prompt import make_llama_3_prompt\n",
    "from util.setup_logging import setup_logging\n",
    "from util.load_dataset import get_dataset \n",
    "from util.get_default_finetune_args import get_default_finetune_args\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "engine = sqlite3.connect(\"SpotifyData.db\")\n",
    "setup_logging()\n",
    "\n",
    "class Args:\n",
    "    \"\"\"\n",
    "    Set-up of arguments with defaults\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 max_examples=100, \n",
    "                 sql_model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                 file_name=\"test-set.jsonl\",\n",
    "                 training_file_name=\"generated_queries.jsonl\",\n",
    "                 num_to_generate=10):\n",
    "        self.sql_model_name = sql_model_name\n",
    "        self.max_examples = max_examples\n",
    "        self.file_name = file_name\n",
    "        self.training_file_name = training_file_name\n",
    "        self.num_to_generate = num_to_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryStage(GenerationNode):\n",
    "    \"\"\"\n",
    "    Generates SQL queries based on natural language prompts.\n",
    "    This class extends the `GenerationNode` class and provides methods for generating SQL queries plus running them on a database to evaluate their success.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(\n",
    "            model_name=model_name,\n",
    "            max_new_tokens=300,\n",
    "        )\n",
    "\n",
    "    def generate( \n",
    "        self,\n",
    "        prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate SQL query based on prompt. Sets output type to a SQLite query.\n",
    "        \"\"\"\n",
    "        results = super().generate(\n",
    "            prompt,\n",
    "            output_type={\"sqlite_query\": \"str\"},\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return results\n",
    "\n",
    "\n",
    "    def postprocess(self, obj: PromptObject):\n",
    "        \"\"\"\n",
    "        Run both the generated and reference SQL queries to assess whether the SQL queries succeeded in hitting the database (not correctness yet)\n",
    "        \"\"\"\n",
    "        \n",
    "        query_succeeded = False\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"Running SQL query '{obj.response['sqlite_query']}'\")\n",
    "            obj.data[\"generated_query\"] = obj.response[\"sqlite_query\"]\n",
    "            df = pd.read_sql(obj.response[\"sqlite_query\"], con=engine)\n",
    "            obj.data['df'] = df\n",
    "            logger.info(f\"Got data: {df}\")\n",
    "            query_succeeded = True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"Failed to run SQL query: {obj.response['sqlite_query']}\"\n",
    "            )\n",
    "\n",
    "        logger.info(f\"Running reference SQL query '{obj.data['sql']}'\")\n",
    "        df = pd.read_sql(obj.data[\"sql\"], con=engine)\n",
    "        logger.info(f\"Got data: {df}\")\n",
    "        obj.data['reference_df'] = df\n",
    "\n",
    "        logger.info(f\"For question: {obj.data['question']}\")\n",
    "        logger.info(f\"For query: {obj.response['sqlite_query']}\")\n",
    "\n",
    "        obj.data[\"query_succeeded\"] = query_succeeded\n",
    "\n",
    "    def preprocess(self, obj: PromptObject):\n",
    "        \"\"\"\n",
    "        Define full user and system prompts using make_prompt() and make_llama_3_prompt() and assign to PromptObject\n",
    "        \"\"\"\n",
    "        new_prompt = make_llama_3_prompt(**self.make_prompt(obj.data))\n",
    "        obj.prompt = new_prompt\n",
    "\n",
    "    def make_prompt(self, data: dict):\n",
    "        \"\"\"\n",
    "        Returns a dictionary with user and system prompts\n",
    "        \"\"\"\n",
    "        system = \"You are a data analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "        system += \"Consider the songs table with the following schema:\\n\"\n",
    "        system += get_schema_altered() + \"\\n\"\n",
    "        system += \"Only return the data required to answer the question in the SQL, don't include other columns or rows. \\n\" #Excluded in 1st iteration\n",
    "        system += (\n",
    "            \"Write a sqlite SQL query that would help you answer the following question. Make sure each query ends with a semicolon:\\n\"\n",
    "        )\n",
    "        user = data[\"question\"]\n",
    "        return {\n",
    "            \"user\": user,\n",
    "            \"system\": system,\n",
    "        }\n",
    "\n",
    "#########################################\n",
    "\n",
    "class ScoreStage(GenerationNode):\n",
    "    \"\"\"\n",
    "    Evaluates the similarity between LLM-generated and reference SQL queries.\n",
    "    This class extends the `GenerationNode` class and provides methods for comparing the dataframes generated by SQL queries. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            max_new_tokens=150,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates an explanation and similarity score for the given prompt\n",
    "        \"\"\"\n",
    "        results = super().generate(\n",
    "            prompt,\n",
    "            output_type={\"explanation\": \"str\", \"similar\": [\"true\", \"false\"]},\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def preprocess(self, obj: PromptObject):\n",
    "        \"\"\"\n",
    "        Prepares the prompt for the evaluation model (comparison of the 2 dataframes) by defining the system and user prompts\n",
    "        \"\"\"\n",
    "        obj.prompt = make_llama_3_prompt(**self.make_prompt(obj))\n",
    "        logger.info(f\"Scoring Stage Prompt:\\n{obj.prompt}\")\n",
    "\n",
    "    def postprocess(self, obj: PromptObject):\n",
    "        \"\"\"\n",
    "        Updates the `PromptObject` with the evaluation results, including whether the dataframes are similar and an explanation\n",
    "        \"\"\"\n",
    "        obj.data['is_matching'] = self.is_matching(obj.data, obj.response)\n",
    "        obj.data['explanation'] = obj.response[\"explanation\"]\n",
    "        obj.data['similar'] = obj.response[\"similar\"] == \"true\"\n",
    "\n",
    "    def is_matching(self, data, response):\n",
    "        \"\"\"\n",
    "        Determines whether the dataframes from the LLM and the test set are the same or similar\n",
    "        \"\"\"\n",
    "        return (str(data.get('df',\"None\")).lower() == str(data['reference_df']).lower() \n",
    "                or response['similar'] == \"true\")\n",
    "\n",
    "    def make_prompt(self, obj: PromptObject): \n",
    "\n",
    "        \"\"\"\n",
    "        The evaluation model compares SQL output from the LLM-generated and reference SQL queries, using another LLM in the pipeline\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = \"Compare the following two dataframes. They are similar if they are almost identical, or if they convey the same information about the songs dataset\"\n",
    "        system_prompt += \"Respond with valid JSON {'explanation' : str, 'similar' : bool}\"\n",
    "        user_prompt = (\n",
    "            f\"========== Dataframe 1 =========\\n{str(obj.data.get('df','None')).lower()}\\n\\n\"\n",
    "        )\n",
    "        user_prompt += (\n",
    "            f\"========== Dataframe 2 =========\\n{str(obj.data['reference_df']).lower()}\\n\\n\"\n",
    "        )\n",
    "        user_prompt += f\"Can you tell me if these dataframes are similar?\"\n",
    "        return {\n",
    "            \"system\": system_prompt,\n",
    "            \"user\": user_prompt\n",
    "        }\n",
    "\n",
    "#######################################################\n",
    "\n",
    "async def run_eval(dataset, args):\n",
    "    \"\"\"\n",
    "    Asynchronously runs run_evaluation_pipeline and prints the length of the results\n",
    "    \"\"\"\n",
    "\n",
    "    results = await run_evaluation_pipeline(dataset, args)\n",
    "\n",
    "    print(\"Total results:\", len(results))\n",
    "\n",
    "    return results\n",
    "\n",
    "async def run_evaluation_pipeline(dataset, args):\n",
    "    \"\"\"\n",
    "    Asynchronously runs the evaluation pipeline.\n",
    "    Returns a list of evaluation results.\n",
    "    \"\"\"\n",
    "    results = EvaluationPipeline(args).call(dataset)\n",
    "\n",
    "    result_list = []\n",
    "\n",
    "    pbar = tqdm(desc=\"Saving results\", unit=\" results\") #creates a progress bar using the tqdm library\n",
    "    async for result in results:\n",
    "        result_list.append(result)\n",
    "        pbar.update()\n",
    "    return result_list\n",
    "\n",
    "#######################################################\n",
    "\n",
    "class EvaluationPipeline(GenerationPipeline): \n",
    "    \"\"\"\n",
    "    Parent class: https://github.com/lamini-ai/lamini/blob/main/lamini/generation/generation_pipeline.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.query_stage = QueryStage(args.sql_model_name) #generating queries\n",
    "        self.score_stage = ScoreStage() #evaluating queries\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Serves as the processing flow, passing the data through the query_stage and then the score_stage.\n",
    "        Called via the \"call\" method in EvaluationPipeline instansiation during run_evaluation_pipeline()\n",
    "        \"\"\"\n",
    "        x = self.query_stage(x)\n",
    "        x = self.score_stage(x)\n",
    "        return x\n",
    "\n",
    "#######################################################\n",
    "    \n",
    "def load_dataset(args):\n",
    "    \"\"\"\n",
    "    Loads original example queries file (test-set-expanded.jsonl) and creates a PromptObject for each data item\n",
    "    \"\"\"\n",
    "    \n",
    "    path = f\"data/{args.file_name}\"\n",
    "\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for index, obj in enumerate(reversed(list(reader))):\n",
    "            if index >= args.max_examples:\n",
    "                break\n",
    "            yield PromptObject(prompt=\"\", data=obj)\n",
    "\n",
    "def save_eval_results(results, args):\n",
    "    \"\"\"\n",
    "    Saves the evaluation results to files\n",
    "    \"\"\"\n",
    "    base_path = \"./data/results\"\n",
    "    now = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    experiment_name = f\"spotify_sql_pipeline_{now}\"\n",
    "    experiment_dir = os.path.join(base_path, experiment_name)\n",
    "    os.makedirs(os.path.join(base_path, experiment_name))\n",
    "\n",
    "    # Write args to file\n",
    "    args_file_name = f\"{experiment_dir}/args.txt\"\n",
    "    with open(args_file_name, \"w\") as writer:\n",
    "        pprint(args.__dict__, writer)\n",
    "\n",
    "    def is_correct(r):\n",
    "        if (\n",
    "            (result.data[\"query_succeeded\"] and result.data['is_matching']) or \n",
    "            result.data[\"generated_query\"] == result.data['sql']\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Write sql results and errors to file\n",
    "    results_file_name = f\"{experiment_dir}/sql_results.jsonl\"\n",
    "    with jsonlines.open(results_file_name, \"w\") as writer:\n",
    "        for result in results:\n",
    "            if not is_correct(result):\n",
    "                continue\n",
    "            writer.write(\n",
    "                {\n",
    "                    \"question\": result.data['question'],\n",
    "                    \"query\": result.data[\"generated_query\"],\n",
    "                    \"query_succeeded\": result.data[\"query_succeeded\"],\n",
    "                    \"reference_sql\": result.data['sql'],\n",
    "                    \"df\": str(result.data.get('df', 'None')),\n",
    "                    \"reference_df\": str(result.data['reference_df']),\n",
    "                    'is_matching': result.data['is_matching'],\n",
    "                    'similar': result.data['similar'],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    results_file_name = f\"{experiment_dir}/sql_errors.jsonl\"\n",
    "    with jsonlines.open(results_file_name, \"w\") as writer:\n",
    "        for result in results:\n",
    "            if is_correct(result):\n",
    "                continue\n",
    "            writer.write(\n",
    "                {\n",
    "                    \"question\": result.data['question'],\n",
    "                    \"query\": result.data[\"generated_query\"],\n",
    "                    \"query_succeeded\": result.data[\"query_succeeded\"],\n",
    "                    \"df\": str(result.data.get('df', 'None')),\n",
    "                    \"reference_df\": str(result.data['reference_df']),\n",
    "                    'is_matching': result.data['is_matching'],\n",
    "                    'similar': result.data['similar'],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Write statistics to file\n",
    "    average_sql_succeeded = sum(\n",
    "        [result.data[\"query_succeeded\"] for result in results]\n",
    "    ) / len(results)\n",
    "    average_correct = sum(\n",
    "        [result.data[\"query_succeeded\"] and result.data['is_matching'] for result in results]\n",
    "    ) / len(results)\n",
    "\n",
    "    file_name = f\"{experiment_dir}/summary.txt\"\n",
    "    with open(file_name, \"w\") as writer:\n",
    "        print(f\"Total size of eval dataset: {len(results)}\", file=writer)\n",
    "        print(f\"Total size of eval dataset: {len(results)}\")\n",
    "        print(f\"Percent Valid SQL Syntax: {average_sql_succeeded*100}\", file=writer)\n",
    "        print(f\"Percent Valid SQL Syntax: {average_sql_succeeded*100}\")\n",
    "        print(f\"Percent Correct SQL Query: {average_correct*100}\", file=writer)\n",
    "        print(f\"Percent Correct SQL Query: {average_correct*100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM-generation of SQL and evaluation of SQL\n",
    "\n",
    "args = Args()\n",
    "dataset = load_dataset(args)\n",
    "results = await run_eval(dataset, args) \n",
    "save_eval_results(results, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelStage(GenerationNode):\n",
    "    \"\"\"\n",
    "    Generates SQL queries to create training data for LLM fine-tuning, using an LLM\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            max_new_tokens=300,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add_template() called to add in Llama 3 prompt tokens and the rest of the prompt.\n",
    "        LLM results returned as \"explanation\", \"sql_query_1\", \"sql_query_2\"\n",
    "        \"\"\"\n",
    "        prompt = self.add_template(prompt)\n",
    "\n",
    "        results = super().generate(\n",
    "            prompt,\n",
    "            output_type={\n",
    "                \"explanation\": \"str\",\n",
    "                \"sql_query_1\": \"str\",\n",
    "                \"sql_query_2\": \"str\",\n",
    "            },\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    async def add_template(self, prompts):\n",
    "        \"\"\"\n",
    "        Creates prompt and adds in prompt tokens for Llama 3 models\n",
    "        \"\"\"\n",
    "        async for prompt in prompts:\n",
    "            new_prompt = make_llama_3_prompt(**self.make_prompt(prompt.data))\n",
    "            yield PromptObject(prompt=new_prompt, data=prompt.data)\n",
    "\n",
    "    async def process_results(self, results):\n",
    "        \"\"\"\n",
    "        Iterates through the generated results: filters out invalid or empty responses and\n",
    "        extracts valid SQL queries\n",
    "        \"\"\"\n",
    "        \n",
    "        async for result in results:\n",
    "            if result is None:\n",
    "                continue\n",
    "\n",
    "            if result.response is None:\n",
    "                continue\n",
    "\n",
    "            logger.info(\"=====================================\")\n",
    "            logger.info(f\"Generated query 1: {result.response['sql_query_1']}\")\n",
    "            logger.info(f\"Generated query 2: {result.response['sql_query_2']}\")\n",
    "            logger.info(\"=====================================\")\n",
    "\n",
    "            if self.check_sql_query(result.response[\"sql_query_1\"]):\n",
    "                new_result = PromptObject(prompt=\"\", data=copy.deepcopy(result.data))\n",
    "                new_result.data.generated_sql_query = result.response[\"sql_query_1\"]\n",
    "                yield new_result\n",
    "\n",
    "            if self.check_sql_query(result.response[\"sql_query_2\"]):\n",
    "                new_result = PromptObject(prompt=\"\", data=copy.deepcopy(result.data))\n",
    "                new_result.data.generated_sql_query = result.response[\"sql_query_2\"]\n",
    "                yield new_result\n",
    "\n",
    "    def make_prompt(self, data):\n",
    "        \"\"\"\n",
    "        Create prompt that creates more SQL queries\n",
    "        \"\"\"\n",
    "        \n",
    "        system = \"You are a data analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "        system += (\n",
    "            \"Consider a table called 'songs' with the following schema (columns). \\n\"\n",
    "        )\n",
    "        system += get_schema_altered()\n",
    "        system += \"Only return the data required to answer the question in the SQL, don't include other columns or rows. \\n\"\n",
    "        system += \"Consider the following questions, and queries used to answer them:\\n\"\n",
    "        for example in data.sample:\n",
    "            system += \"Question: \" + example[\"question\"] + \"\\n\"\n",
    "            system += \"Query: \" + example[\"sql\"] + \"\\n\"\n",
    "        \n",
    "        user = \"Write two queries that are similar but different to those above.\\n\"\n",
    "        user += \"Format the queries as a JSON object, i.e.\\n\"\n",
    "        user += '{ \"explanation\": str, \"sql_query_1\" : str, \"sql_query_2\": str }.\\n'\n",
    "\n",
    "        user += \"First write an explanation of why you decided to write these new queries in about 3-5 sentences, then write valid sqlite SQL queries for each of the 2 new queries. Make sure each query is complete and ends with a ;\\n\"\n",
    "\n",
    "        return {\"system\": system, \"user\": user}\n",
    "\n",
    "    def check_sql_query(self, query):\n",
    "        \"\"\"\n",
    "        Checks validity of SQL against database\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pd.read_sql(query, con=engine)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error in SQL query: {e}\")\n",
    "            return False\n",
    "\n",
    "        logger.info(f\"SQL query {query} is valid\")\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionStage(GenerationNode):\n",
    "    \"\"\"\n",
    "    Generates a question, given a query\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            max_new_tokens=150,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns an explanation and a question, to match the SQL\n",
    "        \"\"\"\n",
    "        results = super().generate(\n",
    "            prompt,\n",
    "            output_type={\n",
    "                \"explanation\": \"str\",\n",
    "                \"question\": \"str\",\n",
    "            },\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def preprocess(self, obj: PromptObject):\n",
    "        \"\"\"\n",
    "        Creates prompt and adds in prompt tokens for Llama 3 models\n",
    "        \"\"\"\n",
    "        new_prompt = make_llama_3_prompt(**self.make_question_prompt(obj.data))\n",
    "        obj.prompt = new_prompt\n",
    "\n",
    "    def make_question_prompt(self, data):\n",
    "        \"\"\"\n",
    "        Creates the prompt for the question creation. Uses Chain of thought (CoT) prompting. \n",
    "        \"\"\"\n",
    "        system = \"You are a data analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "        system += (\n",
    "            \"Consider a table called 'songs' with the following schema (columns)\\n\"\n",
    "        )\n",
    "        system += get_schema() + \"\\n\"\n",
    "        system += \"Queries, and questions that they are used to answer:\\n\"\n",
    "        for example in data.sample:\n",
    "            system += \"Query: \" + example[\"sql\"] + \"\\n\"\n",
    "            system += \"Question: \" + example[\"question\"] + \"\\n\"\n",
    "\n",
    "        user = \"Now consider the following query.\\n\"\n",
    "        user += \"Query: \" + data.generated_sql_query + \"\\n\"\n",
    "        user += \"Write a question that this query could be used to answer.\\n\"\n",
    "\n",
    "        user += \"Format your response as a JSON object, i.e.\\n\"\n",
    "        user += '{ \"explanation\": str, \"question\": str }.\\n'\n",
    "\n",
    "        user += \"First write an explanation in about 3-5 sentences, then write a one sentence question.\\n\"\n",
    "\n",
    "        return {\"system\": system, \"user\": user}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryGenPipeline(GenerationPipeline):\n",
    "    \"\"\"\n",
    "    Represents a pipeline for generating SQL queries from a given dataset.\n",
    "    It consists of two stages: a model stage and a question stage.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_stage = ModelStage()\n",
    "        self.question_stage = QuestionStage()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passes the input through the model stage and then the question stage,\n",
    "        returning the final processed output.\n",
    "        \"\"\"\n",
    "        x = self.model_stage(x)\n",
    "        x = self.question_stage(x)\n",
    "        return x\n",
    "\n",
    "async def run_query_gen_pipeline(dataset_queries):\n",
    "    \"\"\"\n",
    "    Runs the query generation pipeline\n",
    "    \"\"\"\n",
    "    return QueryGenPipeline().call(dataset_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate N samples, for every example in the dataset\n",
    "\n",
    "all_examples = []\n",
    "\n",
    "async def load_queries(args):\n",
    "    \"\"\"\n",
    "    Reads the original user-created SQL from a file and randomly samples the queries for each generation round\n",
    "    \"\"\"\n",
    "    path = f\"data/{args.file_name}\" #i.e. original user-created set of queries\n",
    "\n",
    "    with jsonlines.open(path) as reader:\n",
    "        global all_examples\n",
    "\n",
    "        all_examples = [obj for obj in reader]\n",
    "\n",
    "    sample_count = args.num_to_generate #number of examples to generate per round\n",
    "    sample_size = 3\n",
    "\n",
    "    random.seed(42)\n",
    "\n",
    "    for i in range(sample_count):\n",
    "        example_sample = ExampleSample(random.sample(all_examples, sample_size), i)\n",
    "        yield PromptObject(prompt=\"\", data=example_sample)\n",
    "\n",
    "\n",
    "class ExampleSample:\n",
    "    \"\"\"\n",
    "    Represents the sampled subset of user-generated queries\n",
    "    \"\"\"\n",
    "    def __init__(self, sample, index):\n",
    "        self.sample = sample\n",
    "        self.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def save_generation_results(results, args):\n",
    "    \"\"\"\n",
    "    Saves the generated questions and SQL to a file\n",
    "    \"\"\"\n",
    "    path = f\"data/training_data/{args.training_file_name}\"\n",
    "\n",
    "    pbar = tqdm(desc=\"Saving results\", unit=\" results\") #progress bar\n",
    "    with jsonlines.open(path, \"w\") as writer:\n",
    "\n",
    "        async for result in results:\n",
    "            writer.write(\n",
    "                {\n",
    "                    \"question\": result.response[\"question\"],\n",
    "                    \"sql\": result.data.generated_sql_query,\n",
    "                }\n",
    "            )\n",
    "            pbar.update()\n",
    "\n",
    "        for example in all_examples:\n",
    "            writer.write(example)\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training data ie more SQL and questions for LLM fine-tuning, using LLM. \n",
    "\n",
    "args = Args()\n",
    "dataset_queries = load_queries(args)\n",
    "results = await run_query_gen_pipeline(dataset_queries)\n",
    "await save_generation_results(results, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set-up for fine tuning using the extra LLM-generated data\n",
    "\n",
    "def make_question(obj):\n",
    "    system = \"You are a data analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "    system += \"Consider the 'song' table with the following schema:\\n\"\n",
    "    system += get_schema_altered() + \"\\n\"\n",
    "    system += \"Only return the data required to answer the question in the SQL, don't include other columns or rows. \\n\"\n",
    "    system += (\n",
    "        \"Write a sqlite SQL query that would help you answer the following question:\\n\"\n",
    "    )\n",
    "    user = obj[\"question\"]\n",
    "    return {\"system\": system, \"user\": user}\n",
    "\n",
    "args = Args()\n",
    "llm = lamini.Lamini(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "dataset = get_dataset(args, make_question)\n",
    "\n",
    "finetune_args = get_default_finetune_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tuning\n",
    "\n",
    "\"\"\"\n",
    "llm.train(\n",
    "    data_or_dataset_id=dataset,\n",
    "    finetune_args=finetune_args,\n",
    "    is_public=False,  # For sharing\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of fine-tuned model against original user-created dataset\n",
    "\n",
    "args = Args(sql_model_name=\"place_holder\")\n",
    "dataset = load_dataset(args)\n",
    "results = await run_eval(dataset, args)\n",
    "save_eval_results(results, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
